{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d524f160",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# NLP Notes\n",
    "\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "Vectorization is a fundamental concept in machine learning and is used in various domains beyond just natural language processing (NLP). It's a technique for converting data, including text, images, and more, into numerical vectors that machine learning algorithms can work with. Here are some examples of how vectorization is used in different areas of machine learning:\n",
    "\n",
    "NLP (Natural Language Processing):\n",
    "\n",
    "Word Embeddings: In NLP, words and documents are often represented as vectors through techniques like Word2Vec, GloVe, or FastText. These word vectors capture semantic relationships between words and are used in various NLP tasks, such as sentiment analysis, machine translation, and document classification.\n",
    "Computer Vision:\n",
    "\n",
    "Image Feature Extraction: Images are converted into numerical vectors by extracting features using techniques like Histogram of Oriented Gradients (HOG) or Convolutional Neural Networks (CNNs). These vectors are used for tasks like image classification, object detection, and facial recognition.\n",
    "Recommendation Systems:\n",
    "\n",
    "User and Item Embeddings: In recommendation systems, user and item data can be vectorized to represent preferences and characteristics. Matrix factorization and collaborative filtering techniques are often used to create user-item interaction matrices, which are then factorized to obtain vector representations.\n",
    "Tabular Data:\n",
    "\n",
    "Feature Engineering: In traditional machine learning, numerical and categorical data are transformed into feature vectors. This involves techniques like one-hot encoding for categorical variables and standardization for numerical variables. These feature vectors are used in tasks like regression and classification.\n",
    "Time Series Analysis:\n",
    "\n",
    "Time Series Decomposition: Time series data can be decomposed into components like trend, seasonality, and noise, and each component can be represented as vectors. These components are crucial for time series forecasting and anomaly detection.\n",
    "Audio Processing:\n",
    "\n",
    "Spectrogram Analysis: Audio signals can be converted into spectrograms, which are essentially 2D arrays of numerical values representing the frequency and time. Machine learning models can be applied to these spectrograms for tasks like speech recognition and music classification.\n",
    "Graph Data:\n",
    "\n",
    "Graph Embeddings: In graph-based data, nodes and edges can be vectorized to perform tasks like node classification, link prediction, and community detection. Graph neural networks (GNNs) are commonly used for this purpose.\n",
    "So, vectorization is a versatile technique used to convert data from various domains into a format that machine learning models can process. It's not limited to NLP but is a fundamental aspect of many machine learning applications, making it a crucial part of data preprocessing and feature engineering in the field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93eb7a6",
   "metadata": {},
   "source": [
    "https://neptune.ai/blog/vectorization-techniques-in-nlp-guide#:~:text=In%20Machine%20Learning%2C%20vectorization%20is,converting%20text%20to%20numerical%20vectors.\n",
    "\n",
    "\n",
    "## What is vectorization? \n",
    "\n",
    "Vectorization is jargon for a classic approach of converting input data from its raw format (i.e. text ) into vectors of real numbers which is the format that ML models support. This approach has been there ever since computers were first built, it has worked wonderfully across various domains, and it’s now used in NLP.\n",
    "In Machine Learning, vectorization is a step in feature extraction. The idea is to get some distinct features out of the text for the model to train on, by converting text to numerical vectors.'\n",
    "\n",
    "There are plenty of ways to perform vectorization, as we’ll see shortly, ranging from naive binary term occurrence features to advanced context-aware feature representations. Depending on the use-case and the model, any one of them might be able to do the required task. \n",
    "\n",
    "Let’s learn about some of these techniques and see how we can use them.\n",
    "\n",
    "###  Vectorization techniques\n",
    "\n",
    "1. Bag of Words\n",
    "Most simple of all the techniques out there. It involves three operations:\n",
    "\n",
    "Tokenization (See below)\n",
    "First, the input text is tokenized. A sentence is represented as a list of its constituent words, and it’s done for all the input sentences.\n",
    "\n",
    "Vocabulary creation\n",
    "Of all the obtained tokenized words, only unique words are selected to create the vocabulary and then sorted by alphabetical order.\n",
    "\n",
    "Vector creation\n",
    "Finally, a sparse matrix is created for the input, out of the frequency of vocabulary words. In this sparse matrix, each row is a sentence vector whose length (the columns of the matrix) is equal to the size of the vocabulary.\n",
    "\n",
    "Let’s work with an example and see how it looks in practice. We’ll be using the Sklearn library for this exercise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdac88",
   "metadata": {},
   "source": [
    "\n",
    "https://neptune.ai/blog/tokenization-in-nlp\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "The first thing you need to do in any NLP project is text preprocessing. Preprocessing input text simply means putting the data into a predictable and analyzable form. It’s a crucial step for building an amazing NLP application.\n",
    "\n",
    "There are different ways to preprocess text: \n",
    "\n",
    "stop word removal, \n",
    "tokenization, \n",
    "stemming. \n",
    "\n",
    "Among these, the most important step is tokenization. It’s the process of breaking a stream of textual data into words, terms, sentences, symbols, or some other meaningful elements called tokens. A lot of open-source tools are available to perform the tokenization process. \n",
    "\n",
    "### Why Do We Need Tokenization?\n",
    "\n",
    "Tokenization is a crucial first step in any Natural Language Processing (NLP) process. It plays a significant role in shaping the entire NLP workflow. Here's why it's so important:\n",
    "\n",
    "1. Structuring Text:\n",
    "\n",
    "Tokenization is like taking a jumbled pile of words and making sense of them. It breaks down unstructured text into smaller, meaningful chunks. Think of it as dividing a book into sentences, paragraphs, and words, so a computer can understand it.\n",
    "\n",
    "2. Creating Numerical Data:\n",
    "\n",
    "Computers love numbers, but they struggle with raw text. Tokenization converts words into numbers. Each word becomes a token with a unique number, and these tokens can be counted. This numeric representation immediately makes the text suitable for machine learning.\n",
    "\n",
    "3. Useful Actions:\n",
    "\n",
    "Once text is tokenized, computers can use these tokens to trigger actions. For example, if you tokenize a sentence like \"Turn on the lights,\" a computer can understand that it's a command to control lights and take action accordingly.\n",
    "\n",
    "4. Machine Learning:\n",
    "\n",
    "Tokenized text can also be used as features in machine learning. For example, you can use token counts to predict sentiments, classify documents, or make complex decisions. Tokens become the building blocks for teaching machines to understand and work with language.\n",
    "In a nutshell, tokenization transforms messy, unstructured text into a structured, numerical format. This enables computers to both understand and act upon the text, making it a fundamental step in the NLP journey.\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Input Sentence: \"Tokenization is crucial for NLP.\"\n",
    "\n",
    "The tokenization process would break this sentence down into individual tokens, like this:\n",
    "\n",
    "Tokens:\n",
    "\n",
    "\"Tokenization\"\n",
    "\"is\"\n",
    "\"crucial\"\n",
    "\"for\"\n",
    "\"NLP\"\n",
    "\".\"\n",
    "Each word in the sentence has become a separate token, and even the punctuation (the period at the end) is treated as a token. These tokens are now in a format that a computer can understand and work with. This makes it easier to perform various NLP tasks such as counting the frequency of words, analyzing sentiment, or classifying the text. Tokenization is the first step that allows the computer to make sense of and process human language effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb87e99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['affects',\n",
       " 'are',\n",
       " 'at',\n",
       " 'coronavirus',\n",
       " 'disease',\n",
       " 'due',\n",
       " 'high',\n",
       " 'highly',\n",
       " 'infectious',\n",
       " 'is',\n",
       " 'most',\n",
       " 'older',\n",
       " 'people',\n",
       " 'risk',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## What does a vector look like in python?\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "sents = ['coronavirus is a highly infectious disease',\n",
    "   'coronavirus affects older people the most',\n",
    "   'older people are at high risk due to this disease']\n",
    "\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#Now let’s vectorize our input and convert it into a NumPy array for viewing purposes.\n",
    "\n",
    "X = cv.fit_transform(sents)\n",
    "X = X.toarray()\n",
    "\n",
    "sorted(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae8aa65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ef851",
   "metadata": {},
   "source": [
    "\n",
    "## Bag of Words\n",
    "\n",
    "The \"Bag of Words\" (BoW) is a fundamental concept in Natural Language Processing (NLP). It's a simple and commonly used technique for text analysis and document classification. Here's how it works:\n",
    "\n",
    "Text to Tokens: The first step is to take a piece of text and break it down into individual words or tokens. Each word becomes a separate entity in the process.\n",
    "\n",
    "Counting Word Frequencies: Next, for each token in the text, you count how many times it appears in the document. Essentially, you create a tally of word frequencies. For example, in the sentence \"I love NLP, and NLP is fun,\" the word \"NLP\" appears twice, so its count is 2.\n",
    "\n",
    "Create a Vector: After counting word frequencies, you represent the text as a vector. This vector is typically very high-dimensional, with each dimension representing a unique word from the text. The value in each dimension is the count of how many times the corresponding word appears in the text.\n",
    "\n",
    "Ignoring Word Order: The name \"Bag of Words\" comes from the fact that this technique completely disregards the order in which words appear in the text. It treats the words as an unordered \"bag,\" focusing only on their frequencies.\n",
    "\n",
    "Sparse Vectors: Since most texts contain a large number of unique words, BoW vectors tend to be sparse, meaning they have many dimensions with values of zero.\n",
    "\n",
    "Bag of Words is useful for several NLP tasks, such as:\n",
    "\n",
    "Text Classification: It's often used to represent text documents and then train machine learning models to classify them into categories (e.g., spam or not spam, sentiment analysis, topic classification).\n",
    "\n",
    "Information Retrieval: BoW vectors can help in finding documents that are most relevant to a search query.\n",
    "\n",
    "Document Clustering: It's used to group similar documents together based on the similarity of their word frequencies.\n",
    "\n",
    "However, BoW has limitations, especially in capturing the semantic meaning and context of words. It doesn't consider word order or the relationships between words. More advanced techniques like TF-IDF (Term Frequency-Inverse Document Frequency) and Word Embeddings (e.g., Word2Vec and GloVe) address some of these limitations by capturing more semantic information in text.\n",
    "\n",
    "Say you have bad car or good car,  bag of words doesn't do a good job picking up those kinds of semantic meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e334dab",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Term Frequency\n",
    "\n",
    "In Natural Language Processing (NLP), \"Term Frequency\" (TF) is a numerical statistic that represents how often a term (a word or a token) appears in a document or a piece of text. It's a simple, basic concept used to quantify the importance of a term within a document.\n",
    "\n",
    "The Term Frequency (TF) of a term is typically calculated using the following formula:\n",
    "\n",
    "\n",
    "TF = number of times a word appears in a document - 1 / number of tearms in the document - 1\n",
    "\n",
    "IDF(word) = log(number of documents/number of documents containing the word (word))\n",
    "\n",
    "\n",
    "Calculating the weight of a word using TF-IDF:\n",
    "\n",
    "TF * IDF\n",
    "\n",
    "\n",
    "In this formula:\n",
    "\n",
    "\"t\" represents the specific term you're interested in.\n",
    "\"d\" represents the document in which you're measuring the term frequency.\n",
    "\"Number of times term t appears in document d\" is the count of how many times the term appears in the document.\n",
    "\"Total number of terms in document d\" is the total count of all the terms (words or tokens) in the document.\n",
    "The result of this calculation is a relative measure of how frequently the term appears in the document compared to all the other terms. TF provides information about the local importance of a term within a specific document.\n",
    "\n",
    "Term Frequency is often used in conjunction with another concept called **\"Inverse Document Frequency\" (IDF)** to calculate a combined metric known as TF-IDF (Term Frequency-Inverse Document Frequency). TF-IDF helps in quantifying the importance of a term not just within a single document but also in the context of an entire collection of documents. This combined metric is widely used for information retrieval, text classification, and search engines to determine the relevance of terms in documents and queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed7a80",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "What is the size of the vocabulary?\n",
    "\n",
    "The size of the vocabulary in Natural Language Processing (NLP) depends on the specific dataset and the application. In NLP, the vocabulary typically refers to the total number of unique words or tokens that appear in the corpus (collection of texts) being analyzed. The size of the vocabulary can vary widely based on factors like the dataset's source, domain, and language.\n",
    "\n",
    "Here are some considerations regarding vocabulary size:\n",
    "\n",
    "Dataset Size: Larger datasets tend to have larger vocabularies because they encompass a broader range of words and phrases. For example, a dataset containing all the text on the internet will have a much larger vocabulary than a small collection of scientific articles.\n",
    "\n",
    "Language: Different languages have different vocabularies. English, for example, has a vast vocabulary compared to some other languages. This will influence the vocabulary size when working with text in a specific language.\n",
    "\n",
    "Domain: The domain or subject matter of the text also affects vocabulary size. A medical dataset will have medical terminology that might not be present in a general news dataset, for instance.\n",
    "\n",
    "Data Preprocessing: Tokenization, stemming, and lemmatization techniques can affect the vocabulary size. Some of these techniques combine words or reduce them to their root form, which can reduce the number of unique tokens.\n",
    "\n",
    "Stop Words: The inclusion or exclusion of common \"stop words\" like \"the,\" \"a,\" and \"in\" can impact vocabulary size. These words are often removed to reduce noise in NLP tasks.\n",
    "\n",
    "Rare Words and Out-of-Vocabulary Words: Depending on how rare or specific the words are in the dataset, there might be many out-of-vocabulary words that are not included in the vocabulary. Handling these words can be important in NLP.\n",
    "\n",
    "Word Embeddings: In some NLP applications, word embeddings are used to represent words as dense vectors. These embeddings often have a fixed dimension, so they can reduce the vocabulary size when compared to one-hot encoded representations.\n",
    "\n",
    "To estimate the size of the vocabulary for a specific NLP task, you can perform a preliminary analysis by tokenizing the text and counting unique tokens. Keep in mind that vocabulary size is a trade-off between capturing as much information as possible and managing computational resources, so it's often a balance that depends on the specific goals of your NLP project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b78ba9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Comparing Bag of Words and TF-IDF\n",
    "\n",
    "\"Bag of Words\" (BoW) and \"TF-IDF\" (Term Frequency-Inverse Document Frequency) are both techniques used in Natural Language Processing (NLP), but they serve different purposes and have distinct characteristics. Here's a comparison of the two:\n",
    "\n",
    "### Bag of Words (BoW):\n",
    "\n",
    "Representation: BoW represents a document as a vector, where each dimension of the vector corresponds to a unique word in the entire dataset, and the value in each dimension is the count of how many times that word appears in the document.\n",
    "\n",
    "Word Importance: BoW doesn't take into account the importance of individual words in the document. It treats all words as equally important. no semantic information\n",
    "\n",
    "Word Order: BoW completely disregards the word order and the context of words in the document. It treats the text as an unordered collection of words.\n",
    "\n",
    "it creates a set of vectors containing the count of word occurances in the document\n",
    "\n",
    "disadvantage - depends on the count of words and emphasizes words with high frequency, so important words that have a lower frequency are considered unvaluabel\n",
    "\n",
    "Scalability: The BoW approach can lead to very high-dimensional, sparse vectors, especially in datasets with a large vocabulary. This can be computationally expensive and may require dimensionality reduction techniques.\n",
    "\n",
    "### TF-IDF (Term Frequency-Inverse Document Frequency) (SUPERVISED LEARNING):\n",
    "\n",
    "Representation: TF-IDF represents a document as a vector, but unlike BoW, it assigns a weight to each word. This weight considers both the word's frequency within the document (Term Frequency) and its importance in the entire dataset (Inverse Document Frequency).\n",
    "\n",
    "Word Importance: TF-IDF takes into account the importance of words in a document relative to their importance across a collection of documents. Rare words that are important in a specific document receive higher TF-IDF scores. It contains info on the more important and less important words alike\n",
    "\n",
    "Word Order: Like BoW, TF-IDF also disregards word order and context. It focuses on the importance of individual words.\n",
    "\n",
    "normalizes the count where each word is divided by the number of documents the word appears in\n",
    "\n",
    "Scalability: TF-IDF can also result in high-dimensional vectors, but it often leads to sparser vectors than BoW, as it downweights common words (high IDF) and emphasizes rare words (low IDF).\n",
    "\n",
    "Comparison:\n",
    "\n",
    "Purpose: BoW is primarily used for text classification and document retrieval based on word frequency, while TF-IDF is designed to assess the importance of words in a document relative to their importance across a dataset.\n",
    "\n",
    "Word Importance: TF-IDF assigns weights that reflect the significance of words in a document, making it more effective at capturing the relevance of terms.\n",
    "\n",
    "Use Cases: BoW can be sufficient for simple tasks like spam detection, where word presence/absence matters. TF-IDF is more suitable for tasks requiring a nuanced understanding of the importance of words, such as content recommendation, search engines, and information retrieval.\n",
    "\n",
    "Sparse Vectors: TF-IDF vectors tend to be sparser than BoW vectors because of the IDF component, which can be advantageous in reducing computational complexity.\n",
    "\n",
    "In summary, BoW and TF-IDF are both text vectorization techniques, but they differ in their approach to representing and assessing the importance of words in documents. The choice between them depends on the specific NLP task and the level of sophistication required to capture the text's meaning and relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1fea83",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Text Classification\n",
    "\n",
    "\n",
    "Text classification in Natural Language Processing (NLP) is the process of categorizing or labeling a given piece of text into predefined categories or classes. It's a fundamental task in NLP and has a wide range of applications across various domains. The primary goal of text classification is to automate the assignment of categories to text documents based on their content. Here's a more detailed explanation:\n",
    "\n",
    "Key Elements of Text Classification:\n",
    "\n",
    "Text Input: Text classification takes text documents, such as emails, news articles, customer reviews, social media posts, or any other textual data as input.\n",
    "\n",
    "Predefined Categories: It involves predefined categories or labels into which the text must be classified. These categories are determined based on the specific application, such as spam or not spam for email classification, sentiment (positive, negative, neutral), topic classification, and more.\n",
    "\n",
    "Training Data: To build a text classifier, you typically need a labeled dataset. This dataset contains a large number of text documents where each document is associated with the correct category label. Machine learning algorithms learn from this data to make predictions on new, unseen documents.\n",
    "\n",
    "How Text Classification Works:\n",
    "\n",
    "Data Preprocessing: The first step is to preprocess the text data. This includes tokenization (breaking text into words or tokens), removing stop words, stemming or lemmatization, and often converting text to lowercase for consistency.\n",
    "\n",
    "Feature Extraction: Text documents are transformed into numerical representations. Common techniques include using Bag of Words (BoW), TF-IDF (Term Frequency-Inverse Document Frequency), or word embeddings.\n",
    "\n",
    "Model Training: A machine learning model, such as a classifier (e.g., Naive Bayes, Support Vector Machine, or deep learning models like Convolutional Neural Networks or Recurrent Neural Networks), is trained on the preprocessed and feature-extracted data using the labeled dataset. The model learns to identify patterns and relationships between text features and their corresponding labels.\n",
    "\n",
    "Model Evaluation: The trained model is evaluated on a separate dataset to assess its performance in terms of accuracy, precision, recall, F1-score, or other relevant metrics. This evaluation helps in fine-tuning the model and assessing its generalization to new, unseen data.\n",
    "\n",
    "Inference: Once the model is trained and evaluated, it can be used to classify new, unlabeled text documents into their respective categories.\n",
    "\n",
    "Applications of Text Classification:\n",
    "\n",
    "### Text classification has a wide range of applications, including:\n",
    "\n",
    "* Email spam detection. \n",
    "* Sentiment analysis in social media or product reviews.  \n",
    "* topic labeling \n",
    "* spam detection \n",
    "* intent detection \n",
    "* Language identification.\n",
    "* News categorization.\n",
    "* Legal document categorization.\n",
    "* Customer support ticket routing.\n",
    "* Content recommendation.\n",
    "* Identifying toxic or offensive content in online communities.\n",
    "    \n",
    "In summary, text classification is a critical task in NLP that automates the categorization of text documents into predefined classes, enabling a wide variety of applications where automated content sorting and classification are required.\n",
    "\n",
    "\n",
    "There are 2 methods of text classification:\n",
    "1. manual\n",
    "2. sutomatic\n",
    "\n",
    "\n",
    "#### Machine learning text classification leanrs to make a classification based on past observation\n",
    "#### Rule-based approach classifies the text into groups by using a specific set of defined linguistic rules\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f04a1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Tools from Sentiment Classification\n",
    "\n",
    "### Textblob (UNSUPERVISED LEARNING)\n",
    "\n",
    "TextBlob is a popular Python library for processing and analyzing textual data, primarily in the field of Natural Language Processing (NLP). It provides a simple and convenient API for common NLP tasks, including text classification, sentiment analysis, part-of-speech tagging, and more.\n",
    "\n",
    "One of the most notable features of TextBlob is its built-in sentiment analysis capabilities. Sentiment analysis, also known as opinion mining, is the task of determining the sentiment or emotional tone expressed in a piece of text, such as positive, negative, or neutral. Here's how TextBlob is used for sentiment classification in NLP.\n",
    "\n",
    "textblob.sentiment module contains two sentiment analysis implementations:\n",
    "\n",
    "1. PatternAnalyzer (default)\n",
    "2. NaiveBayesAnalyzer\n",
    "\n",
    "\n",
    "### VADER  (UNSUPERVISED LEARNING)\n",
    "\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool designed for sentiment classification in Natural Language Processing (NLP). It is specifically tailored to analyze and classify sentiment in text, particularly social media content, short text snippets, and informal language commonly found on platforms like Twitter and Facebook. VADER is known for its ability to handle text with mixed sentiments, emoticons, slang, and other nuances that might pose challenges for other sentiment analysis methods.\n",
    "\n",
    "Here's how VADER works and how it's used in sentiment classification:\n",
    "\n",
    "1. Lexicon-Based Approach:\n",
    "VADER relies on a pre-built lexicon of words and phrases, each of which is associated with a sentiment polarity score. These scores range from -1 (extremely negative) to 1 (extremely positive), with 0 indicating neutrality. The lexicon includes thousands of words and phrases, and each has an associated sentiment score based on human judgment and agreement.\n",
    "\n",
    "2. Rule-Based Analysis:\n",
    "VADER uses a set of rules to combine the sentiment scores of individual words and phrases and to account for sentiment intensity, negations, and modifiers. For example, it can recognize intensifiers (e.g., \"very\"), negations (e.g., \"not\"), and capitalization (e.g., \"LOVE\" for emphasis) to refine its sentiment analysis.\n",
    "\n",
    "3. Valence Shifters:\n",
    "VADER also considers valence shifters, which are words that can change the sentiment of the surrounding text. For instance, \"but\" in the sentence \"I like this, but it's too expensive\" signals a shift from positive to negative sentiment.\n",
    "\n",
    "4. Sentiment Polarity and Intensity:\n",
    "VADER produces two primary outputs for a given text snippet: polarity and intensity. The polarity score is the sentiment label, ranging from negative, neutral, to positive, and the intensity score indicates the strength of the sentiment.\n",
    "\n",
    "5. How to Use VADER:\n",
    "To use VADER for sentiment classification in NLP, you typically follow these steps:\n",
    "\n",
    "Import the VADER library (e.g., **nltk.sentiment.vader** in NLTK for Python).\n",
    "Create a VADER sentiment analyzer object.\n",
    "Analyze the sentiment of the text by passing it to the analyzer.\n",
    "Retrieve the polarity and intensity scores to determine the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb89455",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Techniches when cleaning the scripts for analysis\n",
    "\n",
    "#### Lemmitization\n",
    "\n",
    "The provided code is a common preprocessing step in Natural Language Processing (NLP) for text data. It is used to clean and prepare text data for further analysis. Let's break down what this code does:\n",
    "\n",
    "review appears to be a list of words, likely representing a single text document or sentence.\n",
    "\n",
    "lm.lemmatize(word) is a function call to a lemmatization method, which reduces words to their base or dictionary form. For example, it would convert \"running\" to \"run.\"\n",
    "\n",
    "stopwords.words('english') is a list of common stop words in the English language. Stop words are words that are commonly used in a language but often don't provide meaningful information for analysis, such as \"the,\" \"and,\" \"is,\" etc.\n",
    "\n",
    "for word in review if not word in stopwords.words('english') is a list comprehension that iterates through each word in the review list and checks if the word is not in the list of English stopwords. If it's not a stop word, it lemmatizes the word using lm.lemmatize(word).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac8bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # removing the stopwords and creating lemma words to each word present in the each row\n",
    "    review = [lm.lemmatize(word) for word in review if not word in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47678e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Removing accented words\n",
    "\n",
    "    # removing the accented words\n",
    "    review = [unidecode.unidecode(word) for word in review]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REmove special characters\n",
    "\n",
    "    # removing the special characetrs, numbers from the data\n",
    "    review = re.sub('[^a-zA-Z]', ' ', data['review'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b859ab",
   "metadata": {},
   "source": [
    "\n",
    "## SPARSE VECTOR MAtrices\n",
    "\n",
    "\n",
    "\n",
    "In Natural Language Processing (NLP), a sparse vector is a vector in which most of its elements are zero or very close to zero. This is in contrast to a dense vector, where most of its elements have non-zero values. Sparse vectors are common in NLP because they are used to represent the presence or absence of words or features in a large vocabulary or feature space.\n",
    "\n",
    "Here's why sparse vectors are prevalent in NLP:\n",
    "\n",
    "High Dimensionality: NLP tasks often involve a large vocabulary of words or a high-dimensional feature space. Representing each word or feature using a dense vector would result in a massive amount of memory and computational requirements.\n",
    "\n",
    "Efficiency: Many NLP models and algorithms, like text classifiers and topic models, rely on linear algebra operations. Operating with sparse vectors can be much more efficient because most elements are zero, so the computations can focus on the non-zero elements.\n",
    "\n",
    "Storage: Sparse vectors require significantly less storage space compared to dense vectors. This is crucial when dealing with large text corpora or datasets.\n",
    "\n",
    "Dimension Reduction: Techniques like TF-IDF and LSA (Latent Semantic Analysis) result in sparse vector representations of text data, allowing for dimensionality reduction while preserving semantic information.\n",
    "\n",
    "To illustrate, consider a vocabulary with tens of thousands of words. If you represent a document as a dense vector in this space, you would need to allocate memory for each word, most of which will be zero (indicating the absence of that word). Using a sparse vector, you only store the non-zero elements along with their corresponding indices, saving memory and computation.\n",
    "\n",
    "In summary, sparse vectors are widely used in NLP to represent text data efficiently in high-dimensional spaces. They are a crucial component of various NLP models, allowing for the effective handling of large vocabularies and datasets.\n",
    "\n",
    "\n",
    "\n",
    "### PRoblems with Sparse VEctors\n",
    "\n",
    "While sparse vectors are an efficient and necessary representation in many Natural Language Processing (NLP) tasks, they come with certain challenges and drawbacks. Here are some of the issues and problematic aspects associated with sparse vectors:\n",
    "\n",
    "Storage and Memory Overhead: Sparse vectors can reduce storage requirements compared to dense vectors, but they still consume memory for the non-zero elements and their indices. For extremely high-dimensional spaces, this can still result in significant memory overhead.\n",
    "\n",
    "Computational Complexity: Operations involving sparse vectors, such as vector addition and multiplication, can be less efficient than operations on dense vectors. Sparse vectors require specialized algorithms for efficient processing, and these algorithms might be more complex and slower than their dense counterparts.\n",
    "\n",
    "Sparsity Impact: The high sparsity of vectors means that most elements are zero or close to zero. As a result, the information contained in a sparse vector is heavily skewed towards the non-zero elements. This can make it challenging to capture fine-grained relationships between features, especially when the data is highly dimensional.\n",
    "\n",
    "Loss of Local Context: In some NLP tasks, the sparsity of vectors can lead to a loss of local context. Sparse representations may not effectively capture the context of words or features in a document, as they often rely on single-word or single-feature occurrences.\n",
    "\n",
    "Curse of Dimensionality: High-dimensional spaces with sparse vectors can be subject to the \"curse of dimensionality.\" In such spaces, data can become more sparse, making it challenging to find meaningful patterns or relationships, especially with limited data.\n",
    "\n",
    "Dimensionality Reduction Challenges: When applying dimensionality reduction techniques to sparse data, information loss can be significant. While techniques like LSA and PCA are commonly used, they might not preserve all the essential semantic information present in the original data.\n",
    "\n",
    "Handling Missing Data: In many real-world NLP applications, not all data will have a representation for every word or feature. Dealing with missing data can be complex, as the absence of information can lead to difficulties in model training and inference.\n",
    "\n",
    "Despite these challenges, sparse vectors remain essential in NLP due to their efficiency and ability to handle high-dimensional data. Researchers and practitioners often work with strategies and techniques to mitigate these issues and extract meaningful information from sparse representations. Additionally, advancements in hardware and software continue to improve the efficiency of sparse vector operations, making them a viable choice in NLP applications.\n",
    "\n",
    "\n",
    "\n",
    "## DEnse Encoding\n",
    "\n",
    "\n",
    "In Natural Language Processing (NLP), dense encodings, also known as dense vector representations or embeddings, are numerical representations of words, phrases, or other linguistic units in a continuous and high-dimensional vector space. Dense encodings are in contrast to sparse encodings, which are binary or highly sparse (containing many zero values). Dense encodings aim to capture the meaning, context, and relationships between words or phrases in a more compact and continuous form.\n",
    "\n",
    "Deals with high dimensionality\n",
    "\n",
    "They represent semantics\n",
    "\n",
    "Key characteristics of dense encodings in NLP include:\n",
    "\n",
    "Continuous Values: Dense encodings are represented as vectors of real numbers, and each dimension in the vector has a continuous value. This contrasts with sparse encodings, where most values are zero.\n",
    "\n",
    "Semantic Information: Dense encodings are designed to capture semantic information. Words or phrases with similar meanings or usage tend to have similar or closely located vectors in the vector space. This enables models to understand and reason about word meanings and relationships.\n",
    "\n",
    "Distributional Semantics: Dense encodings are based on the distributional hypothesis, which posits that words that appear in similar contexts have similar meanings. This concept, often referred to as distributional semantics, underlies the creation of dense word embeddings.\n",
    "\n",
    "Word Vectors: Word embeddings, such as Word2Vec, GloVe, and FastText, are common examples of dense encodings in NLP. These models learn to represent words as high-dimensional vectors where words with related meanings are close in the vector space.\n",
    "\n",
    "Pre-trained Models: Pre-trained word embeddings are available, which have been trained on large text corpora. These embeddings capture the general semantic information of words, phrases, or even sentences. They can be used in various NLP tasks and fine-tuned for specific applications.\n",
    "\n",
    "Deep Learning Models: Dense encodings are often used in deep learning models for NLP tasks. These models, such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), and transformers, work with dense vector representations of text data to capture and learn complex patterns and relationships.\n",
    "\n",
    "Dense encodings have significantly improved the performance of NLP models and tasks because they enable more effective handling of semantics and context. They have been instrumental in the development of state-of-the-art NLP models for tasks like text classification, sentiment analysis, machine translation, and more. The availability of pre-trained word embeddings has made it easier for researchers and practitioners to leverage this technology for various NLP applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8c0587",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### List of Dense Encoding Models\n",
    "\n",
    "Several popular dense encoding models have been developed for Natural Language Processing (NLP). Each of these models aims to learn dense vector representations (embeddings) of words, phrases, or text data, capturing semantic and contextual information. Here are some of the most widely used dense encoding models and their key characteristics:\n",
    "\n",
    "Word2Vec:\n",
    "\n",
    "Word2Vec is one of the foundational models for dense word embeddings. It includes two architectures: Continuous Bag of Words (CBOW) and Skip-gram.\n",
    "CBOW predicts a target word based on its context words, while Skip-gram predicts context words given a target word.\n",
    "Word2Vec learns word vectors in a way that words with similar meanings or usage are located close to each other in the vector space.\n",
    "GloVe (Global Vectors for Word Representation):\n",
    "\n",
    "GloVe is another popular model for word embeddings. It focuses on global word-to-word co-occurrence statistics in a corpus.\n",
    "It captures the relationships between words by considering the likelihood of words appearing together in a text corpus.\n",
    "FastText:\n",
    "\n",
    "FastText, developed by Facebook AI Research, extends the concept of word embeddings to subword or character-level embeddings.\n",
    "It can represent out-of-vocabulary words by breaking them down into subword components, making it more robust for morphologically rich languages.\n",
    "ELMo (Embeddings from Language Models):\n",
    "\n",
    "ELMo is a deep contextualized word representation model. It uses a deep bidirectional LSTM to generate contextualized embeddings for words.\n",
    "ELMo captures word meaning in different contexts, as word embeddings are generated dynamically based on the surrounding words.\n",
    "BERT (Bidirectional Encoder Representations from Transformers):\n",
    "\n",
    "BERT is a transformer-based model that pre-trains on a large corpus of text and can be fine-tuned for various downstream NLP tasks.\n",
    "BERT captures bidirectional contextual information and is considered a breakthrough in NLP, achieving state-of-the-art results in many tasks.\n",
    "GPT (Generative Pre-trained Transformer):\n",
    "\n",
    "GPT is a series of transformer-based models developed by OpenAI. GPT-3, in particular, is known for its large-scale pre-training on diverse data and impressive language generation capabilities.\n",
    "GPT models can be fine-tuned for specific NLP tasks.\n",
    "T5 (Text-to-Text Transfer Transformer):\n",
    "\n",
    "T5 is a transformer-based model that treats all NLP tasks as text-to-text problems. It performs exceptionally well across a wide range of NLP tasks by transforming them into a consistent format.\n",
    "RoBERTa (A Robustly Optimized BERT Pretraining Approach):\n",
    "\n",
    "RoBERTa is an optimized version of BERT that focuses on hyperparameter tuning and scaling to achieve state-of-the-art performance in various NLP tasks.\n",
    "XLNet (eXtreme Learning with a Transformer):\n",
    "\n",
    "XLNet is another variant of the transformer model that uses a permutation-based approach for pre-training, allowing it to model context more effectively.\n",
    "Each of these models has contributed to significant advancements in NLP by learning dense vector representations of text data that capture semantic and contextual information. These embeddings have been widely adopted in various NLP applications, including text classification, sentiment analysis, machine translation, named entity recognition, and more. The choice of which model to use depends on the specific task and requirements of a project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a9322d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Word2vec\n",
    "\n",
    "Word2Vec is a popular and powerful word embedding technique in the field of Natural Language Processing (NLP). It's a shallow, two-layer neural network that learns to represent words as continuous-valued vectors in a high-dimensional space. These word vectors, often called word embeddings, capture semantic and contextual information about words, allowing machines to better understand and work with human language.\n",
    "\n",
    "It takes a text corpus as input and returns a set of vectors known as feature vectors that represent words in that corpus.\n",
    "\n",
    "Word2vec itself is NOT a DNN, but it converts text to a numerical format that ML models can understand.\n",
    "\n",
    "Here are the key components and concepts of Word2Vec:\n",
    "\n",
    "1. Word Embeddings:\n",
    "Word2Vec learns distributed representations of words, where each word is represented as a vector of real numbers. These word vectors have the property that words with similar meanings or usage tend to have similar vector representations. This concept is known as distributional semantics.\n",
    "\n",
    "2. Two Architectures:\n",
    "Word2Vec includes two main architectures for training word vectors: Continuous Bag of Words (CBOW) and Skip-gram.\n",
    "\n",
    "CBOW (Continuous Bag of Words): Given the context words (words that surround a target word), CBOW aims to predict the target word. It's like filling in the blank given the context. This is often used for smaller datasets.\n",
    "\n",
    "Skip-gram: Given a target word, Skip-gram aims to predict the context words. It's like understanding the context from a single word. Skip-gram is more widely used and often performs better on large datasets.\n",
    "\n",
    "3. Training Objective:\n",
    "Word2Vec trains its word vectors by maximizing the likelihood of predicting context words (in Skip-gram) or target words (in CBOW) from the given words. The training objective seeks to make the predicted words as close as possible to the actual words in the training data.\n",
    "\n",
    "4. Semantic Similarity:\n",
    "Word2Vec captures semantic similarity, meaning that words with similar meanings or usage have similar vector representations. This allows you to perform operations like vector arithmetic (e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\") and find words that are contextually related.\n",
    "\n",
    "5. Applications:\n",
    "Word2Vec word embeddings are widely used in NLP tasks, such as text classification, sentiment analysis, named entity recognition, machine translation, and recommendation systems. They can improve the performance of these tasks by providing better representations of words and phrases.\n",
    "\n",
    "6. Pre-trained Models:\n",
    "Pre-trained Word2Vec models are available, which have been trained on large text corpora. These pre-trained embeddings can be used in various NLP projects without the need for training from scratch.\n",
    "\n",
    "Word2Vec is a fundamental technique in NLP that has greatly improved the ability of machines to understand and work with human language. It has been a significant advancement in the field and remains a crucial component in many NLP applications.\n",
    "\n",
    "#### Notes\n",
    "\n",
    "* Problem with word2vec is the inability to handle unknown or out-of-vocabulary (OOV) words.  If your model hasn't encountered a word before, it will have no idea how to interpret it or how to build a vector for it.  you then have to use a random vector, which is not ideal\n",
    "\n",
    "* no shared representations at sub-word levels.  If word2vec ecnouncers a new word which ends in \"less\", like flawless, there will be no like between less and flawless.\n",
    "\n",
    "* word2vec represents every word as and independent vector, even though some words are similar\n",
    "\n",
    "* scaling to new languages requires new embedding matrices and does not allow for parameter sharing meaning cross-linquistical use of the same model isn't an option\n",
    "\n",
    "* word2vec also requires a large corpus to train - through at this point in time in the industry large volumnes are abundantly available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51069a3e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Continuous Bag of Words (CBOW)\n",
    "\n",
    "Continuous Bag of Words (CBOW) is one of the two main architectures used in the Word2Vec model, a popular word embedding technique in Natural Language Processing (NLP). CBOW is designed to learn word representations by predicting a target word based on the surrounding context words in a text corpus. It's particularly useful for capturing the context and meaning of words.\n",
    "\n",
    "CBOW is trianed to predict a single word from a fixed window size of context words.\n",
    "\n",
    "In CBOW, to predict a target word, the sum of the vector background vectors is used.\n",
    "\n",
    "The CBOW model calculates the average of the context vectors.\n",
    "\n",
    "Each word in first transformed into a one-hot encoding form. aslo only particular words that are in a window will be taken into consideration rather than all the words in the sentence\n",
    "\n",
    "\n",
    "\n",
    "Here's how CBOW works:\n",
    "\n",
    "1. Sliding Window Context:\n",
    "\n",
    "In CBOW, a sliding window approach is used to define the context of a word. This window of context words includes a fixed number of words (typically symmetrically positioned around the target word).\n",
    "2. Objective Function:\n",
    "\n",
    "The primary objective of CBOW is to predict the target word (center word) from the surrounding context words. The context words are used as input features for predicting the target word.\n",
    "3. Neural Network Architecture:\n",
    "\n",
    "CBOW uses a simple feedforward neural network architecture with one hidden layer. The input layer consists of the one-hot encoded vectors of the context words, and the output layer consists of the predicted word's one-hot encoded vector.\n",
    "4. Weight Matrix:\n",
    "\n",
    "The neural network has a weight matrix connecting the input layer to the hidden layer. This weight matrix is trained to capture the relationships between context words and the target word. The hidden layer processes the context words and produces an embedding vector.\n",
    "5. Projection Layer:\n",
    "\n",
    "The embedding vector from the hidden layer is projected onto the output layer using another weight matrix. This projection layer transforms the embedding vector into a probability distribution over all the words in the vocabulary.\n",
    "6. Softmax Activation:\n",
    "\n",
    "CBOW uses a softmax activation function in the output layer to compute the probabilities of each word in the vocabulary being the target word.\n",
    "7. Training:\n",
    "\n",
    "CBOW is trained using a large dataset, and the objective is to maximize the likelihood of predicting the target word given the context words. This involves adjusting the weights in the neural network using gradient descent optimization.\n",
    "The resulting word vectors, or word embeddings, obtained after training the CBOW model, capture the semantic and contextual information of words. Words that share similar contexts in the training data will have similar vector representations, allowing for efficient modeling of semantic relationships between words.\n",
    "\n",
    "CBOW has its advantages, including faster training on smaller datasets and being more useful when you need context for a word. However, its counterpart, the Skip-gram model, is often favored for larger datasets and tasks where understanding context from a single word is important. Both architectures have contributed significantly to improving NLP applications and tasks by providing effective word embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee7cd9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Skipgram\n",
    "\n",
    "Skip-gram is one of the two main architectures used in the Word2Vec model, a popular word embedding technique in Natural Language Processing (NLP). Skip-gram is designed to learn word representations by predicting the context words based on a target word in a text corpus. It is particularly useful for capturing the context and meaning of words.\n",
    "\n",
    "After obtaining the weight matrix the steps to get word embeddings is same as CBOW.\n",
    "\n",
    "Even rare words can be acuratly represented by skip-gram when only a small portion of the training data is used\n",
    "\n",
    "CBOW trains faster than Skip-gram and can represent more frequent words\n",
    "\n",
    "\n",
    "Here's how Skip-gram works:\n",
    "\n",
    "1. Sliding Window Context:\n",
    "\n",
    "In Skip-gram, a sliding window approach is used to define the context of a word. This window of context words includes a fixed number of words (typically symmetrically positioned around the target word).\n",
    "2. Objective Function:\n",
    "\n",
    "The primary objective of Skip-gram is to predict the context words from the target word. The target word is used as input to predict the context words.\n",
    "3. Neural Network Architecture:\n",
    "\n",
    "Skip-gram uses a simple feedforward neural network architecture with one hidden layer. The input layer consists of the one-hot encoded vector of the target word, and the output layer consists of the predicted context words' one-hot encoded vectors.\n",
    "4. Weight Matrix:\n",
    "\n",
    "The neural network has a weight matrix connecting the input layer to the hidden layer. This weight matrix is trained to capture the relationships between the target word and the context words. The hidden layer processes the target word and produces an embedding vector.\n",
    "5. Projection Layer:\n",
    "\n",
    "The embedding vector from the hidden layer is projected onto the output layer using another weight matrix. This projection layer transforms the embedding vector into a probability distribution over all the words in the vocabulary.\n",
    "6. Softmax Activation:\n",
    "\n",
    "Skip-gram uses a softmax activation function in the output layer to compute the probabilities of each word in the vocabulary being a context word given the target word.\n",
    "7. Training:\n",
    "\n",
    "Skip-gram is trained using a large dataset, and the objective is to maximize the likelihood of predicting the context words given the target word. This involves adjusting the weights in the neural network using gradient descent optimization.\n",
    "The resulting word vectors, or word embeddings, obtained after training the Skip-gram model, capture the semantic and contextual information of words. Words that share similar contexts in the training data will have similar vector representations, allowing for efficient modeling of semantic relationships between words.\n",
    "\n",
    "Skip-gram is often favored for larger datasets and tasks where understanding context from a single word is important. It has played a significant role in improving NLP applications and tasks by providing effective word embeddings that capture the nuances of word meanings and relationships.\n",
    "\n",
    "\n",
    "### Negative Sampling\n",
    "\n",
    "Negative sampling is a technique used in the Skip-gram model for training word embeddings in Word2Vec. \n",
    "\n",
    "Negative Sampling allows us to modify a small percentage of the weights rather than all of them for each training sample\n",
    "\n",
    "Its primary purpose is to make the training process more computationally efficient while maintaining the quality of learned word vectors. Instead of using traditional softmax-based training, which involves all the words in the vocabulary, negative sampling focuses on a small subset of words, making the process faster and more scalable. Here's how negative sampling works in the Skip-gram model:\n",
    "\n",
    "1. Traditional Softmax vs. Negative Sampling:\n",
    "\n",
    "In the traditional Skip-gram model with softmax-based training, the output layer of the neural network computes the probability distribution over the entire vocabulary. The network assigns high probabilities to context words that appear with the target word and low probabilities to other words. Computing this distribution can be computationally expensive, especially for large vocabularies.\n",
    "\n",
    "Negative sampling, on the other hand, simplifies this process by considering only a small subset of words for each training example. Instead of predicting the context words directly, negative sampling transforms the task into a binary classification problem, where the goal is to differentiate context words from randomly chosen words that do not appear in the context.\n",
    "\n",
    "2. Negative Samples:\n",
    "\n",
    "For each training example (target word and its context words), a small number of \"negative samples\" are randomly selected from the vocabulary. These negative samples are words that are not in the context of the target word.\n",
    "3. Binary Classification:\n",
    "\n",
    "The neural network is trained to predict whether a given word-context pair (target word and context word) is a true positive (context word) or a negative sample (random word). This is done by maximizing the likelihood of correctly classifying the word pairs.\n",
    "4. Loss Function:\n",
    "\n",
    "Negative sampling typically uses a logistic regression-like loss function, such as the sigmoid cross-entropy loss. The loss function measures the likelihood of a true context word having a high probability while a negative sample has a low probability.\n",
    "5. Computational Efficiency:\n",
    "\n",
    "Negative sampling reduces the computational cost of training, as it avoids the need to compute probabilities over the entire vocabulary. Training becomes more efficient and scalable, making it suitable for working with large corpora and vocabularies.\n",
    "6. Choice of Negative Samples:\n",
    "\n",
    "The choice of how many negative samples to use and how to sample them can be a hyperparameter that you can tune. Typically, you might sample a few (e.g., 5-20) negative examples for each positive context word.\n",
    "Overall, negative sampling in the Skip-gram model reduces the computational complexity of training word embeddings while achieving similar quality results. It is widely used in practice for training word vectors when working with large text corpora and vocabularies.\n",
    "\n",
    "\n",
    "#### Notes for Skip gram\n",
    "\n",
    "Skipgram with negative sampling (SGNs) is slower than CBOW, however performs effectively with a small amount of the training data and accuratly represents even uncommon words or phrases.\n",
    "\n",
    "training involves 2 hyperparameters:\n",
    "1. window size\n",
    "2. number of negative samples\n",
    "\n",
    "smaller window sizes (2-15) produce embeddings with high similarity scores, indicating that the words are interchangeable\n",
    "\n",
    "larger window sizes (15-50, or more) result in embeddings where similarity is more indicative of the words relatedness\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70efd4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Glove - Global VEctors\n",
    "\n",
    "Global Vectors (GloVe), short for \"Global Vectors for Word Representation,\" is a popular word embedding model in Natural Language Processing (NLP). GloVe is designed to learn word vectors or word embeddings, which are dense numerical representations of words, by capturing the global and local word-to-word co-occurrence statistics in a large text corpus. These embeddings are aimed at capturing the semantic meaning and relationships between words based on their usage in the corpus.\n",
    "\n",
    "Key characteristics of GloVe, or global vectors, include:\n",
    "\n",
    "Global Co-occurrence Information: GloVe focuses on the global co-occurrence statistics of words across a large corpus. It computes a global word-word co-occurrence matrix that quantifies how often words appear together in the same context or document.\n",
    "\n",
    "Learning Word Vectors: The model uses this co-occurrence matrix to learn word vectors for each word in the vocabulary. These word vectors are continuous-valued representations, and words with similar co-occurrence patterns have similar vectors.\n",
    "\n",
    "Semantic Information: GloVe word embeddings capture the semantic information of words, allowing words with related meanings to have vectors that are close to each other in the vector space.\n",
    "\n",
    "Dimensionality: Users can choose the dimensionality of the word vectors when training a GloVe model. Common choices include 50, 100, 200, or 300 dimensions.\n",
    "\n",
    "Scalability: GloVe is scalable and can be trained on large text corpora. This makes it a valuable tool for capturing semantic information from extensive text data.\n",
    "\n",
    "Pre-trained Models: Pre-trained GloVe embeddings are available, which have been trained on massive text corpora. These pre-trained embeddings can be used in various NLP tasks, allowing researchers and practitioners to leverage the learned global vectors without the need for training from scratch.\n",
    "\n",
    "Efficiency: The GloVe model offers a compromise between training efficiency and the quality of learned embeddings. It provides good results for many NLP tasks without the computational cost of larger models like transformers.\n",
    "\n",
    "GloVe embeddings have been widely adopted in NLP for a variety of applications, including text classification, sentiment analysis, machine translation, and more. Their ability to capture global co-occurrence statistics and semantic information has made them a valuable resource for improving the understanding of word meanings and relationships in text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfbf7c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Semantic Analysis\n",
    "\n",
    "Semantic analysis, also known as semantic understanding, is a fundamental component of Natural Language Processing (NLP) and linguistics. It involves the interpretation and comprehension of the meaning of words, phrases, sentences, and text in a human language. Semantic analysis aims to extract the underlying meaning, context, and relationships between words and concepts, allowing machines to understand and work with language more effectively.\n",
    "\n",
    "Here's why semantic analysis is important:\n",
    "\n",
    "Language Understanding: Semantic analysis helps machines understand human language at a deeper level, going beyond mere syntactic or grammatical structure. It enables machines to interpret what a sentence or phrase means and extract the intended message.\n",
    "\n",
    "Improved Search and Information Retrieval: In search engines, semantic analysis can help improve search results by considering the meaning and context of search queries and documents. It allows for more accurate and relevant information retrieval.\n",
    "\n",
    "Text Classification and Sentiment Analysis: In text classification tasks, such as spam detection or topic categorization, semantic analysis can help discern the underlying meaning of text. For sentiment analysis, it's crucial in determining the emotional tone of the text.\n",
    "\n",
    "Machine Translation: In machine translation, understanding the semantics of a sentence in one language is essential for accurately translating it into another language. Semantic analysis helps ensure that translations are contextually accurate.\n",
    "\n",
    "Question Answering Systems: In question-answering systems, such as chatbots or virtual assistants, semantic analysis aids in understanding user queries and providing contextually relevant answers.\n",
    "\n",
    "Information Extraction: For information extraction tasks, like named entity recognition or event extraction, semantic analysis helps identify entities, relationships, and events in text.\n",
    "\n",
    "Text Summarization: In text summarization, understanding the semantics of a document is critical for creating concise and coherent summaries that capture the essential information.\n",
    "\n",
    "Language Generation: In natural language generation tasks, such as content generation or chatbot responses, semantic analysis plays a role in generating coherent and contextually appropriate text.\n",
    "\n",
    "Semantics in Search Engines: Search engines use semantic analysis to understand the intent of a search query and to rank web pages based on the relevance and meaning of their content.\n",
    "\n",
    "Interdisciplinary Research: Semantic analysis is crucial for interdisciplinary research that involves analyzing and understanding textual data, such as in social sciences, economics, and healthcare.\n",
    "\n",
    "Language Models and AI: State-of-the-art language models, like BERT and GPT, rely on advanced semantic analysis to understand the context of text and generate coherent and contextually relevant responses. This is important for AI applications like natural language understanding and generation.\n",
    "\n",
    "In essence, semantic analysis is vital for enhancing the capabilities of machines to interact with human language. It enables applications that require understanding, interpretation, and generation of text, making it a foundational component of NLP and AI systems.\n",
    "\n",
    "\n",
    "\n",
    "### Co-occurance matrix\n",
    "\n",
    "\n",
    "A co-occurrence matrix, also known as a co-occurrence table or co-occurrence data, is a data structure used in various fields, including Natural Language Processing (NLP) and information retrieval, to represent how often two elements (typically words) appear together in a given context, such as within a document, a sentence, or a corpus of text. This matrix quantifies the relationships and associations between elements based on their co-occurrence patterns.\n",
    "\n",
    "In the context of NLP and word embeddings, a co-occurrence matrix represents the frequency of words appearing together within a text corpus. Here's how a co-occurrence matrix is typically constructed:\n",
    "\n",
    "Select a Context Window: To build a co-occurrence matrix, you first choose a context window, which is a fixed number of words that are considered to be in the proximity of a target word. For example, if you choose a context window of five words, you consider the five words before and after the target word in a sentence.\n",
    "\n",
    "Count Co-occurrences: For each word in the corpus, you iterate through the text data and count how often it co-occurs within the context window of other words. You increment the co-occurrence count for word pairs that appear together within the chosen context window.\n",
    "\n",
    "Construct the Matrix: The resulting co-occurrence counts are used to populate the co-occurrence matrix. Rows and columns in the matrix represent words, and each cell in the matrix contains the co-occurrence count between the word represented by the row and the word represented by the column.\n",
    "\n",
    "Dimensionality Reduction: To reduce the dimensionality of the co-occurrence matrix and create dense vector representations of words, you can apply techniques such as Singular Value Decomposition (SVD) or matrix factorization.\n",
    "\n",
    "The co-occurrence matrix serves as a fundamental data source for many word embedding models, such as GloVe (Global Vectors for Word Representation), which uses global co-occurrence statistics to learn word vectors. In these models, the co-occurrence matrix is processed to capture semantic relationships between words based on their patterns of occurrence in the text corpus.\n",
    "\n",
    "Co-occurrence matrices are essential in NLP because they provide a quantitative basis for understanding how words relate to each other in context. They form the basis for creating word embeddings and are instrumental in capturing semantic information and relationships between words, which are essential for various NLP tasks like text classification, sentiment analysis, machine translation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20abad1f",
   "metadata": {},
   "source": [
    "\n",
    "## Embedding Layer\n",
    "\n",
    "How the Embedding Layer Works:\n",
    "\n",
    "Mapping Words to Vectors: Suppose you have a sentence: \"I love deep learning.\" In NLP, you typically start by tokenizing the sentence, breaking it down into individual words or tokens: [\"I\", \"love\", \"deep\", \"learning\"]. Each of these tokens is represented as a unique integer based on its position in the vocabulary. For example, \"I\" might be 1, \"love\" could be 2, \"deep\" is 3, and \"learning\" is 4.\n",
    "\n",
    "Embedding Matrix: The embedding layer is initialized with a random or pre-trained embedding matrix. This matrix has dimensions (vocabulary size x embedding dimension), where the vocabulary size is the total number of unique words or tokens in your dataset, and the embedding dimension is a user-defined hyperparameter (e.g., 100, 300, etc.). For example, if you have a vocabulary of 10,000 words and set the embedding dimension to 100, your embedding matrix would be 10,000 x 100.\n",
    "\n",
    "Vector Lookup: To convert the tokens into dense vectors, the embedding layer performs a vector lookup operation. It takes the integer representation of a token, looks up the corresponding row (vector) in the embedding matrix, and returns that vector. So, if \"love\" is represented as 2, the embedding layer would look up the 2nd row in the embedding matrix and return the dense vector associated with \"love.\"\n",
    "\n",
    "Resulting Dense Vector: The result is a dense vector of real values, where each element represents a different aspect or feature of the token. This dense vector encodes the semantic meaning and context of the token within your chosen embedding dimension.\n",
    "\n",
    "Why the Embedding Layer Is Useful:\n",
    "\n",
    "Reduced Dimensionality: The embedding layer reduces the high-dimensional, sparse one-hot encoded representation of words to a lower-dimensional continuous space. For example, if your embedding dimension is 100, each word is represented by a vector of 100 real values, which is far more efficient for neural networks to process than sparse vectors with thousands of zeros and a single 1.\n",
    "\n",
    "Semantic Similarity: Words with similar meanings are often closer to each other in the embedding space. This similarity is learned during training. For example, in the embedding space, the vectors for \"cat\" and \"dog\" will be closer to each other than to unrelated words like \"car\" or \"house.\" This property allows neural networks to capture semantic relationships.\n",
    "\n",
    "Generalization: Embeddings help neural networks generalize better. By learning the semantic relationships between words, models can make reasonable predictions even for words or tokens they haven't seen during training, improving the model's ability to handle out-of-vocabulary words.\n",
    "\n",
    "Parameter Learning: The embedding parameters are updated during training, just like other neural network weights. This means that the embeddings adapt to the specific task the model is trained for, whether it's sentiment analysis, machine translation, or any other NLP task.\n",
    "\n",
    "In summary, embedding layers are crucial in NLP and other areas where categorical data is involved. They convert categorical inputs into dense, continuous vectors that capture semantic information, making it easier for neural networks to learn patterns, generalize, and make sense of the data. These dense, real-valued vectors are much more expressive and informative than one-hot encoded representations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## LSTM\n",
    "\n",
    "Main difference bw LSTM and RNN is that LSTM has memory cells\n",
    "\n",
    "Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNNs) are both types of neural networks used for sequential data processing, such as time series, natural language, and speech. While they share some similarities, LSTMs were designed to address certain limitations of traditional RNNs, and they offer more sophisticated mechanisms for handling long-term dependencies and mitigating the vanishing gradient problem. Here's an explanation of what LSTM is and how it's similar to RNNs:\n",
    "\n",
    "Recurrent Neural Networks (RNNs):\n",
    "\n",
    "Basic Structure: RNNs are a type of neural network designed for processing sequences of data. They have a basic structure where information is passed from one step in the sequence to the next through recurrent connections.\n",
    "\n",
    "Shortcomings: One significant limitation of basic RNNs is the vanishing gradient problem. It can't capture long-range dependencies in data, which is a common issue when working with sequences that have long time lags or dependencies.\n",
    "\n",
    "Long Short-Term Memory (LSTM):\n",
    "\n",
    "Introduction: LSTMs were introduced as a solution to the vanishing gradient problem and to capture long-range dependencies in sequential data.\n",
    "\n",
    "Memory Cells: LSTMs introduce the concept of memory cells, which can store and retrieve information over long sequences. These memory cells have three gates:\n",
    "\n",
    "Forget Gate: Determines what information to forget from the previous cell state.\n",
    "Input Gate: Decides what new information to store in the cell state.\n",
    "Output Gate: Determines what information should be used to make predictions.\n",
    "Long-Term Dependencies: The key advantage of LSTMs is their ability to maintain and propagate information across many time steps, making them well-suited for tasks that require capturing long-term dependencies.\n",
    "\n",
    "Similarities between LSTM and RNN:\n",
    "\n",
    "Sequential Data Processing: Both LSTM and RNN are designed for sequential data. They process data one step at a time and maintain hidden states that capture information from previous steps.\n",
    "\n",
    "Recurrent Connections: Both LSTM and RNN employ recurrent connections that allow information to be passed from one time step to the next.\n",
    "\n",
    "Learning Sequential Patterns: They can learn and model sequential patterns, making them suitable for tasks like time series forecasting, natural language processing, and speech recognition.\n",
    "\n",
    "Differences between LSTM and RNN:\n",
    "\n",
    "Long-Term Dependencies: LSTMs are better at capturing long-range dependencies in data due to their memory cells and gating mechanisms. Basic RNNs struggle with this.\n",
    "\n",
    "Vanishing Gradient: LSTMs are designed to alleviate the vanishing gradient problem, which can hinder training in standard RNNs.\n",
    "\n",
    "Complexity: LSTMs are more complex than basic RNNs due to their additional components (gates and memory cells). This complexity allows them to handle more intricate patterns in data.\n",
    "\n",
    "In summary, LSTMs and RNNs are both used for sequential data, but LSTMs are an improvement over basic RNNs in their ability to handle long-term dependencies and mitigate the vanishing gradient problem. LSTMs achieve this through the use of memory cells and gating mechanisms, making them particularly well-suited for a wide range of tasks in sequential data analysis, including natural language processing and time series modeling.\n",
    "\n",
    "\n",
    "\n",
    "## RNN\n",
    "\n",
    "A Recurrent Neural Network (RNN) is a type of artificial neural network designed for processing sequential data. Unlike traditional feedforward neural networks, where information flows in one direction (from input to output), RNNs have recurrent connections that allow them to maintain a hidden state representing information from previous time steps. This makes them well-suited for tasks involving sequences, such as time series data, natural language, speech recognition, and more.\n",
    "\n",
    "Here's an overview of what an RNN is and what it can be used for:\n",
    "\n",
    "Structure of an RNN:\n",
    "\n",
    "Hidden State: The core feature of an RNN is its hidden state. At each time step in the sequence, the network takes an input and combines it with the hidden state from the previous time step to produce a new hidden state. This allows the network to capture information from past time steps and carry it forward in the sequence.\n",
    "\n",
    "Recurrent Connections: The recurrent connections in RNNs enable them to model temporal dependencies in data. The network's parameters are shared across all time steps, making it capable of handling sequences of varying lengths.\n",
    "\n",
    "Applications of RNNs:\n",
    "\n",
    "RNNs can be used in various applications that involve sequential data:\n",
    "\n",
    "Natural Language Processing (NLP): RNNs are widely used for tasks like language modeling, text generation, machine translation, and sentiment analysis. They can capture the context and dependencies between words in a sentence.\n",
    "\n",
    "Speech Recognition: RNNs are applied to convert audio input into text. They are used in automatic speech recognition (ASR) systems to transcribe spoken language.\n",
    "\n",
    "Time Series Prediction: RNNs can forecast future values in time series data, making them useful for tasks like stock price prediction, weather forecasting, and energy consumption prediction.\n",
    "\n",
    "Video Analysis: RNNs can be used in video analysis for tasks like action recognition, video captioning, and tracking objects in video streams.\n",
    "\n",
    "Music Generation: RNNs can generate music and compose melodies by learning patterns in musical sequences.\n",
    "\n",
    "Anomaly Detection: RNNs are used to detect anomalies in sequential data, such as fraud detection in financial transactions or identifying anomalies in sensor data.\n",
    "\n",
    "Recommender Systems: RNNs can be applied in recommendation systems to model user behavior over time and make personalized recommendations.\n",
    "\n",
    "Robotics: RNNs can control robotic systems, helping them navigate, perform tasks, and adapt to changing environments.\n",
    "\n",
    "Language Modeling: In addition to NLP tasks, RNNs are used in generative language models like LSTM and GRU to create coherent text based on context.\n",
    "\n",
    "It's important to note that while RNNs are effective for many sequential data tasks, they have some limitations. One significant issue is the vanishing gradient problem, which can make it challenging to capture long-term dependencies in data. To address this, more advanced RNN architectures like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) have been developed, which offer improved handling of long-range dependencies.\n",
    "\n",
    "\n",
    "Some advantages of RNN over ANN:\n",
    "1. RNNs are well-suited for processing sequential data, as they store info about the sequence of the data.  This often results in better predictions that ANNs bc ANNs do not have the ability to store hidden states or sequential data\n",
    "2. RNNs are not bound to the size of the input the way ANNs are.  this makes RNNs more generalizable to use cases such as text where the size of the input sequence is not nessisarily fixed\n",
    "3. RNNs use shared parameters/weights unlike ANNs, which require training unique parameters for every connection.  This allows RNNs to reduce the number of parameters that the model needs to learn while training, resulting in computational efficiency\n",
    "\n",
    "\n",
    "## Types of Analysis using recurrence\n",
    "\n",
    "RNNs are flexible in that we can modify the architecture to solve different types of problems relating to sequential data\n",
    "\n",
    "### There are 4 types of RNN archetectures\n",
    "1. one to one (binary classification)\n",
    "2. one to many\n",
    "3. Many to many (named entity recognition (NER), MAchine translation)\n",
    "4. Many to one (music generation, text generation)\n",
    "\n",
    "\n",
    "RNNs don't perform wellon long-term sequential data if Tanh or Relu are used as the activation function in the network.  This is because of the vanishing and exploding gradient issue\n",
    "\n",
    "RNNs also can be slow and complex procedure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7b9ab",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## LSTM\n",
    "\n",
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture that was specifically designed to address the limitations of traditional RNNs, making it more effective at handling sequential data. LSTMs are particularly well-suited for tasks that involve capturing long-term dependencies and mitigating the vanishing gradient problem that plagues standard RNNs.\n",
    "\n",
    "Here's an explanation of what LSTMs are, how they differ from RNNs, and how they are used:\n",
    "\n",
    "Long Short-Term Memory (LSTM):\n",
    "\n",
    "LSTMs are a type of RNN that incorporates memory cells and gating mechanisms. These components allow LSTMs to capture and maintain information over long sequences, making them effective at learning and modeling complex patterns in sequential data.\n",
    "\n",
    "Key Components of LSTMs:\n",
    "\n",
    "Memory Cells: LSTMs have memory cells that can store information over multiple time steps. These cells are at the heart of the LSTM's ability to capture and remember long-term dependencies.\n",
    "\n",
    "Gating Mechanisms: LSTMs employ three gates that control the flow of information in and out of the memory cells:\n",
    "\n",
    "Forget Gate: Determines what information to forget from the previous cell state.\n",
    "Input Gate: Decides what new information to store in the cell state.\n",
    "Output Gate: Determines what information should be used to make predictions.\n",
    "Differences and Advantages of LSTMs over RNNs:\n",
    "\n",
    "Handling Long-Term Dependencies: LSTMs are much better at capturing long-range dependencies in data. Standard RNNs often suffer from the vanishing gradient problem, which makes it difficult for them to propagate error signals back through time steps. LSTMs address this issue with their gating mechanisms, allowing them to learn and maintain information over extended sequences.\n",
    "\n",
    "Preventing Exploding/Vanishing Gradients: LSTMs are more stable during training, which means they are less likely to experience exploding or vanishing gradients. This makes them more robust and easier to train.\n",
    "\n",
    "Modeling Complex Sequences: LSTMs are more effective at modeling complex patterns in sequential data. They are capable of handling tasks like language modeling, machine translation, and speech recognition, where capturing subtle and long-range dependencies is crucial.\n",
    "\n",
    "Flexible and Adaptable: LSTMs can be used for various tasks, from time series forecasting to natural language processing. They can be applied to tasks that involve sequences of varying lengths.\n",
    "\n",
    "Applications of LSTMs:\n",
    "\n",
    "LSTMs are used in a wide range of applications, including:\n",
    "\n",
    "Natural Language Processing (NLP): LSTMs are used for tasks like language modeling, text generation, machine translation, and sentiment analysis.\n",
    "\n",
    "Speech Recognition: LSTMs are applied to convert audio input into text, such as in automatic speech recognition (ASR) systems.\n",
    "\n",
    "Time Series Forecasting: LSTMs can forecast future values in time series data, making them useful for tasks like stock price prediction and weather forecasting.\n",
    "\n",
    "Sequence-to-Sequence Tasks: LSTMs are used for tasks like language translation, where the input and output are both sequences.\n",
    "\n",
    "Generative Models: LSTMs are used to create generative models for text, music, and other sequences, such as LSTM-based language models and recurrent generative adversarial networks (RNNs).\n",
    "\n",
    "In summary, LSTMs are an advanced type of RNN that overcomes the limitations of standard RNNs by effectively handling long-term dependencies and mitigating the vanishing gradient problem. They are widely used in various applications involving sequential data, where modeling complex and long-range dependencies is essential.\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "let's consider two examples where long-term dependencies need to be handled and explore how a standard RNN might have issues with handling them, and how an LSTM could handle the long-term dependency more effectively:\n",
    "\n",
    "Example 1: Language Modeling\n",
    "\n",
    "Scenario: Language modeling involves predicting the next word in a sentence given the previous words. To make an accurate prediction, the model needs to capture long-term dependencies between words.\n",
    "\n",
    "Issue with RNN:\n",
    "\n",
    "In a basic RNN, information from earlier time steps tends to decay rapidly as it propagates through the network. This means that the model may struggle to remember words or phrases that occurred in the distant past of a long sentence. For instance, if a sentence started with \"The cat that was lost,\" a standard RNN might have difficulty remembering \"lost\" when it has to predict the next word.\n",
    "How LSTM Handles It:\n",
    "\n",
    "LSTMs excel in handling long-term dependencies. The memory cells and gating mechanisms in LSTMs allow them to capture information from earlier time steps effectively. In this case, the LSTM would be better at retaining the relevant context, enabling it to predict the next word more accurately, even when there are many words in between.\n",
    "Example 2: Speech Recognition\n",
    "\n",
    "Scenario: Speech recognition systems convert spoken language into text. They need to recognize phonemes and words, which often depend on sounds that occurred earlier in the audio sequence.\n",
    "\n",
    "Issue with RNN:\n",
    "\n",
    "A standard RNN might struggle to capture the dependencies between phonemes and words when they are separated by many other sounds. The vanishing gradient problem can cause the network to forget important acoustic features, leading to poor recognition.\n",
    "How LSTM Handles It:\n",
    "\n",
    "LSTMs are well-suited for speech recognition because they can maintain information about phonemes or words over longer audio segments. The memory cells and gating mechanisms allow LSTMs to hold relevant acoustic features in memory and use them to make more accurate predictions.\n",
    "In both examples, the key advantage of LSTMs is their ability to capture and remember information over extended sequences. This is crucial for tasks that involve understanding and predicting based on long-term dependencies in the data. Standard RNNs are more likely to struggle with these dependencies due to the vanishing gradient problem, making LSTMs a more appropriate choice when handling such scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a159d5ea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## NLP Applications\n",
    "\n",
    "\n",
    "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. NLP has a wide range of applications across various domains. Here are some common NLP applications:\n",
    "\n",
    "Text Classification:\n",
    "\n",
    "MAchine Translations: are used to translate one language to another\n",
    "\n",
    "Sentiment Analysis: Determining the sentiment (positive, negative, neutral) of text, which is used in social media monitoring, product reviews, and brand sentiment analysis.\n",
    "Topic Classification: Categorizing text documents into predefined topics or categories, which is useful for news categorization, content recommendation, and content filtering.\n",
    "Machine Translation:\n",
    "\n",
    "Language Translation: Translating text from one language to another, as seen in popular tools like Google Translate and language localization for software and websites.\n",
    "Information Retrieval:\n",
    "\n",
    "Search Engines: Improving search engine results by understanding user queries and returning relevant documents or web pages.\n",
    "Question-Answering Systems: Developing systems that can understand and answer questions posed in natural language.\n",
    "Named Entity Recognition (NER):\n",
    "\n",
    "Identifying and classifying entities mentioned in text, such as names of people, organizations, locations, dates, and more. This is crucial for information extraction and indexing.\n",
    "Text Generation and Summarization:\n",
    "\n",
    "Text Generation: Generating human-like text for applications like chatbots, virtual assistants, and content creation.\n",
    "Text Summarization: Automatically summarizing long documents or articles to provide concise versions.\n",
    "Speech Recognition:\n",
    "\n",
    "Converting spoken language into text, widely used in voice assistants, transcription services, and hands-free communication.\n",
    "Text-to-Speech (TTS):\n",
    "\n",
    "Converting text into natural-sounding speech, used in applications like audiobooks, navigation systems, and accessibility tools for the visually impaired.\n",
    "Language Understanding and Dialogue Systems:\n",
    "\n",
    "Developing chatbots, virtual assistants, and conversational agents that can understand and respond to user queries in natural language.\n",
    "Language Modeling:\n",
    "\n",
    "Building predictive language models for various tasks, including autocomplete and spell-checking.\n",
    "Text Analytics:\n",
    "\n",
    "Extracting insights and patterns from large volumes of text data, such as social media analytics, market research, and customer feedback analysis.\n",
    "Text Mining and Information Extraction:\n",
    "Extracting structured information from unstructured text, such as extracting data from resumes, news articles, or legal documents.\n",
    "Sentiment Analysis:\n",
    "Analyzing text to determine the sentiment or emotional tone, which is useful for understanding customer feedback, market trends, and brand perception.\n",
    "Authorship Attribution and Plagiarism Detection:\n",
    "Identifying the author of a text or detecting instances of plagiarism in academic and journalistic content.\n",
    "\n",
    "Language Education and Assessment:\n",
    "\n",
    "Developing language learning tools, automated grading systems, and language proficiency assessment tests.\n",
    "Legal and Regulatory Compliance:\n",
    "Analyzing legal documents, contracts, and regulatory texts to ensure compliance and extract relevant information.\n",
    "Healthcare and Biomedical NLP:\n",
    "Analyzing medical records, clinical notes, and biomedical literature for tasks such as disease diagnosis, drug discovery, and patient information extraction.\n",
    "These are just a few examples of the diverse range of NLP applications, and the field continues to evolve, with new applications and advancements regularly emerging. NLP plays a crucial role in making natural language accessible and understandable to computers, enabling a wide array of practical and innovative applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da25750",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Sequential model - LSTM**\n",
    "\n",
    "Sequential models is a common name for Neural Network architectures, that have units with memory. Here we shall be using LSTM, which is a type of Recurrent Neural Network layer as the memory units. \n",
    "\n",
    "A typical LSTM unit is made up of a cell, an input gate, an output gate, and a forget gate. The cell is responsible for \"remembering\" values over extended periods of time and \"forgetting\" certain values when the context changes. This allows LSTM to maintain \"memory.\" Each of the three gates can be viewed as a Fully-Connected Artificial Neural Network, or Multi-Layer Perceptrons. They calculate an activation (using an activation function) of a weighted sum. Additionally, the input and output vectors are accompanied by a context vector that contains information about the summary of the text it has processed thus far.\n",
    "\n",
    "Intuitively, the gates regulate the flow of values that pass through the connections of the LSTM, which is why they are referred to as \"gates.\" They tend to output values between '1' and '0', where '1' is multiplied with the context and input vectors. The values that are multiplied with zero are forgotten and those multiplied with 1 are remembered.\n",
    "\n",
    "\n",
    "**Components of the LSTM**\n",
    "\n",
    "The LSTM cell contains the following components:\n",
    "\n",
    "- Forget Gate “f” ( a neural network with sigmoid)\n",
    "- Candidate layer “C\"(a NN with Tanh)\n",
    "- Input Gate “I” ( a NN with sigmoid )\n",
    "- Output Gate “O”( a NN with sigmoid)\n",
    "- Hidden state “H” ( a vector )\n",
    "- Memory state “C” ( a vector)\n",
    "\n",
    "Inputs to the LSTM cell at any step are Xt (current input) , Ht-1 (previous hidden state ) and Ct-1 (previous memory state).\n",
    "\n",
    "Outputs from the LSTM cell are Ht (current hidden state ) and Ct (current memory state)\n",
    "\n",
    "\n",
    "\n",
    "**Working of gates in LSTM**\n",
    "\n",
    "First, LSTM cell takes the previous memory state Ct-1 and does element wise multiplication with forget gate (f) to decide if present memory state Ct. If forget gate value is 0 then previous memory state is completely forgotten else f forget gate value is 1 then previous memory state is completely passed to the cell ( Remember f gate gives values between 0 and 1 ).\n",
    "\n",
    "Ct = Ct-1 * ft\n",
    "\n",
    "Calculating the new memory state:\n",
    "\n",
    "Ct = Ct + (It * C`t)\n",
    "\n",
    "Now, we calculate the output:\n",
    "\n",
    "Ht = tanh(Ct)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A sequential model is constructed by adding various layers to it.\n",
    "\n",
    "- The first layer is the Embedding layer which transforms one-hot encoded sparse vectors into Word Embedding vectors. As the model continues to train, the weights of the Embedding layer are adjusted so that words with similar meanings are located closer together in the vector space, or have similar Word Embedding Vectors. For example, \"orange\" would be located near \"tangerine\" and \"queen\" would be near \"empress.\" The vocabulary size is specified.\n",
    "\n",
    "- The subsequent layer is an LSTM layer with 150 neurons. The input for this layer is a list of sentences, where each word has been converted to its corresponding Embedding vector and padded to have the same length. The activation function used is ReLU, which is widely used, but other relevant activation functions can also be used.\n",
    "\n",
    "- To prevent bias, a dropout layer is employed to regulate the network.\n",
    "\n",
    "- The final layer is a Dense layer which serves as the output layer and has 2 cells to perform classification, representing the 2 different categories in this example.\n",
    "\n",
    "- The model is then compiled using the Adam optimizer and categorical cross-entropy. The Adam optimizer is currently the best choice for handling sparse gradients and noisy problems, and categorical cross-entropy is typically used when the classes are mutually exclusive, meaning each sample belongs to exactly one class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412197bb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## NMT\n",
    "\n",
    "Neural Machine Translation (NMT) is a state-of-the-art approach in the field of Natural Language Processing (NLP) that is used for automatic language translation. NMT models are designed to translate text from one language to another and have significantly improved the quality and fluency of machine translation systems. Here's an explanation of what NMT is, how it's used, and how it works:\n",
    "\n",
    "What is NMT?\n",
    "\n",
    "NMT is a type of machine translation that employs neural networks, particularly deep learning models, to perform translation tasks. Traditional machine translation systems, like rule-based or statistical machine translation, relied on handcrafted rules and large corpora of parallel text data. In contrast, NMT learns translation patterns directly from data, making it more adaptable and capable of handling multiple language pairs.\n",
    "\n",
    "How NMT is Used:\n",
    "\n",
    "NMT is used in various applications, including:\n",
    "\n",
    "Language Translation: The primary application of NMT is translating text from one language to another. It powers popular translation services and tools like Google Translate.\n",
    "\n",
    "Multilingual Communication: NMT facilitates communication between speakers of different languages, both in written and spoken forms.\n",
    "\n",
    "Global Content Localization: It's used to localize digital content, such as websites, software, and marketing materials, to adapt them for different language-speaking audiences.\n",
    "\n",
    "How NMT Works:\n",
    "\n",
    "NMT models typically consist of an encoder-decoder architecture, and the most commonly used model type is the Sequence-to-Sequence (Seq2Seq) model with attention mechanisms. Here's a high-level overview of how it works:\n",
    "\n",
    "Encoding the Source Text:\n",
    "\n",
    "The input text (source language) is tokenized and passed through an encoder, which is usually a recurrent neural network (RNN) or a more modern choice like the Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU).\n",
    "The encoder processes the input text word by word and produces a context or thought vector, which encodes the semantic meaning of the source text.\n",
    "Attention Mechanism:\n",
    "\n",
    "An attention mechanism allows the model to focus on different parts of the source text while generating the target translation. This is crucial for handling long sentences and ensuring that the model attends to relevant information.\n",
    "Decoding the Target Text:\n",
    "\n",
    "The decoder, another recurrent neural network, takes the context vector produced by the encoder and generates the target translation (output text) one word at a time.\n",
    "At each step, the decoder considers the context vector and the previously generated words to make informed decisions about the next word in the translation.\n",
    "Training the Model:\n",
    "\n",
    "NMT models are trained on parallel corpora of source and target language texts. During training, the model learns to minimize the difference between its predictions and the actual translations in the training data.\n",
    "Inference:\n",
    "\n",
    "During inference (translation), the model uses the learned encoder-decoder weights to generate translations for new source texts.\n",
    "The advantages of NMT over earlier machine translation methods are its ability to capture complex language structures and long-range dependencies, making it more fluent and accurate. Additionally, NMT models can be fine-tuned for specific language pairs and domains, improving translation quality for specialized applications. The availability of pre-trained NMT models, along with techniques like transfer learning, has further accelerated the development and deployment of NMT systems for various languages and domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845db50",
   "metadata": {},
   "source": [
    "\n",
    "### Common Hyperparameters used in LSTM\n",
    "\n",
    "Hyperparameters are crucial for training a Long Short-Term Memory (LSTM) model effectively. Commonly used hyperparameters for LSTM networks include:\n",
    "\n",
    "Number of Units (Neurons): The number of LSTM units in the hidden layer, which determines the model's capacity to learn and remember information. A larger number of units can capture more complex patterns but may increase the risk of overfitting.\n",
    "\n",
    "Dropout: Dropout is a regularization technique to prevent overfitting. You can specify the dropout rate for input and recurrent connections. Typical values range from 0.2 to 0.5.\n",
    "\n",
    "Recurrent Dropout: Similar to dropout, recurrent dropout specifies the dropout rate for recurrent connections in the LSTM layer. Common values are between 0.2 and 0.5.\n",
    "\n",
    "Activation Function: The activation function for LSTM units, which is often set to the default \"tanh.\" You can experiment with other activation functions like \"relu\" or \"sigmoid\" for different use cases.\n",
    "\n",
    "Batch Size: The number of training examples in each batch during training. Batch size affects the speed and memory usage of training. Typical values are 32, 64, or 128.\n",
    "\n",
    "Epochs: The number of times the model goes through the entire training dataset. The right number of epochs depends on your specific task and dataset. Early stopping can be used to prevent overfitting.\n",
    "\n",
    "Learning Rate: The step size for adjusting the model's weights during training. Common learning rates include 0.1, 0.01, or smaller values. You can use adaptive learning rate techniques like Adam or RMSprop to handle learning rates automatically.\n",
    "\n",
    "Loss Function: The loss function that the model optimizes during training. For classification tasks, binary cross-entropy is used for binary classification, and categorical cross-entropy for multiclass classification.\n",
    "\n",
    "Optimizer: The optimization algorithm used for weight updates during training. Common choices include Adam, RMSprop, and SGD.\n",
    "\n",
    "Sequence Length: The length of input sequences or the number of time steps the LSTM is unrolled for. It should match the length of your input data sequences.\n",
    "\n",
    "Embedding Dimension: For text data, the dimension of the word embeddings used in the embedding layer before the LSTM.\n",
    "\n",
    "Vocabulary Size: The maximum number of unique words or tokens in the input text data.\n",
    "\n",
    "Early Stopping: A hyperparameter that determines when to stop training to prevent overfitting. You can set criteria based on validation loss or other metrics.\n",
    "\n",
    "Regularization: L2 regularization can be applied to the LSTM layers to prevent overfitting.\n",
    "\n",
    "Batch Normalization: You can apply batch normalization to stabilize training and improve convergence.\n",
    "\n",
    "Bidirectional LSTM: For some tasks, using bidirectional LSTMs can improve performance. This involves processing the input sequence both forwards and backward.\n",
    "\n",
    "The ideal values for these hyperparameters depend on your specific NLP task, dataset, and problem complexity. A common approach is to perform hyperparameter tuning, experimenting with different values to find the best combination for your particular use case. Additionally, techniques like grid search or random search can be helpful for systematically exploring hyperparameter space.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "User\n",
    "what about number of convolutional layers?\n",
    "ChatGPT\n",
    "The number of convolutional layers is a hyperparameter that primarily applies to Convolutional Neural Networks (CNNs) rather than LSTMs. In CNNs, the number of convolutional layers is a crucial aspect of the network's architecture, and it significantly influences the network's ability to capture hierarchical features in images.\n",
    "\n",
    "However, when combining CNNs with LSTMs in hybrid models for tasks like image captioning or video analysis, you may indeed need to specify the number of convolutional layers in the CNN part of the model.\n",
    "\n",
    "In the context of pure LSTM models for sequential data (such as text or time series), convolutional layers are not typically used. LSTM models are more focused on handling sequences with memory cells and recurrent connections.\n",
    "\n",
    "So, if you are building an LSTM-based model for sequential NLP tasks, you won't typically specify the number of convolutional layers as a hyperparameter. Instead, you'll be working with hyperparameters specific to LSTMs, such as the number of LSTM units, dropout rates, and the architecture of the LSTM layers. If you are using CNNs in combination with LSTMs, you'll need to determine the number of convolutional layers based on your specific image analysis requirements.\n",
    "\n",
    "In summary, the number of convolutional layers is not a direct hyperparameter for LSTM-based models, but it is important for CNNs and models that combine CNNs with LSTMs for tasks like image or video analysis.\n",
    "\n",
    "\n",
    "The number of convolutional layers is a hyperparameter that primarily applies to Convolutional Neural Networks (CNNs) rather than LSTMs. In CNNs, the number of convolutional layers is a crucial aspect of the network's architecture, and it significantly influences the network's ability to capture hierarchical features in images.\n",
    "\n",
    "However, when combining CNNs with LSTMs in hybrid models for tasks like image captioning or video analysis, you may indeed need to specify the number of convolutional layers in the CNN part of the model.\n",
    "\n",
    "In the context of pure LSTM models for sequential data (such as text or time series), convolutional layers are not typically used. LSTM models are more focused on handling sequences with memory cells and recurrent connections.\n",
    "\n",
    "So, if you are building an LSTM-based model for sequential NLP tasks, you won't typically specify the number of convolutional layers as a hyperparameter. Instead, you'll be working with hyperparameters specific to LSTMs, such as the number of LSTM units, dropout rates, and the architecture of the LSTM layers. If you are using CNNs in combination with LSTMs, you'll need to determine the number of convolutional layers based on your specific image analysis requirements.\n",
    "\n",
    "In summary, the number of convolutional layers is not a direct hyperparameter for LSTM-based models, but it is important for CNNs and models that combine CNNs with LSTMs for tasks like image or video analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d025b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "In a machine translation model using LSTMs, encoders play a critical role in processing and representing the source language text before it is translated into the target language. The encoder is responsible for capturing the semantic meaning and context of the input text, which is crucial for producing accurate translations. Here's a detailed explanation of the role of encoders:\n",
    "\n",
    "Input Sequence Processing: The encoder takes the input sequence in the source language and processes it one token (word or subword) at a time. It converts the sequence of tokens into a fixed-length representation, often referred to as the \"context\" or \"thought vector.\" This representation contains information about the entire input sequence.\n",
    "\n",
    "Capturing Context and Dependencies: LSTMs within the encoder capture the context and dependencies between words in the source text. This includes understanding how words relate to each other within the sentence and how they contribute to the overall meaning of the text. The recurrent connections in LSTMs help in modeling long-range dependencies.\n",
    "\n",
    "Embedding Words: The encoder typically includes an embedding layer that converts each token (word) into a continuous vector representation. These word embeddings provide a distributed representation of words in a high-dimensional space, allowing the model to capture the semantic relationships between words.\n",
    "\n",
    "Hidden States: As the encoder processes the input sequence, it maintains a set of hidden states. These hidden states serve as the memory of the model, capturing information from earlier parts of the input sequence. The hidden states are updated at each time step, considering the current input token and the previous hidden state.\n",
    "\n",
    "Contextual Information: The final hidden state of the encoder, often referred to as the \"context vector,\" encapsulates the entire context and meaning of the source text. It is a summary of the input sequence, taking into account the contributions of all words in the sequence.\n",
    "\n",
    "Transfer to Decoder: Once the encoder has processed the entire source language sequence, the context vector is passed to the decoder. The decoder uses this context vector as the initial state to generate the target language translation.\n",
    "\n",
    "The encoder's role is to convert the variable-length source language sequence into a fixed-length representation that captures the essential information and context. This fixed-length representation is used by the decoder to generate a translation, word by word, in the target language. LSTMs in the encoder are particularly effective at handling sequential data and capturing the nuances of language, making them a valuable component in machine translation models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02792b14",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Building Natural Language Processing (NLP) models comes with several challenges, reflecting the complexity and nuances of human language. Here are the top 10 main challenges you may encounter when developing NLP models:\n",
    "\n",
    "Data Quality and Quantity: Collecting and preprocessing high-quality training data is a significant challenge. Noisy, unstructured, or biased data can lead to model errors.\n",
    "\n",
    "Ambiguity and Polysemy: Language is often ambiguous, with words having multiple meanings depending on context. Resolving ambiguity is challenging, especially in context-dependent tasks.\n",
    "\n",
    "Out-of-Vocabulary (OOV) Words: Handling words or phrases not seen during training (OOV words) is important. Proper handling of OOV words is crucial for model robustness.\n",
    "\n",
    "Context Understanding: Capturing context and understanding long-range dependencies in text, especially for tasks like coreference resolution and sentiment analysis, can be difficult.\n",
    "\n",
    "Domain Adaptation: Adapting NLP models to specific domains or jargon, such as medical or legal text, can be challenging due to the unique language and terminology used.\n",
    "\n",
    "Multilingual and Cross-Lingual Challenges: Building models that work across multiple languages or for language pairs with varying structures and grammar requires additional efforts.\n",
    "\n",
    "Lack of Annotated Data: Many NLP tasks require annotated data for supervised learning. Annotating data can be costly and time-consuming.\n",
    "\n",
    "Bias and Fairness: NLP models can perpetuate societal biases present in training data. Ensuring fairness and mitigating bias is an ongoing concern.\n",
    "\n",
    "Model Interpretability: Understanding and explaining why a model makes certain predictions is crucial for trust and accountability, but it remains a challenge.\n",
    "\n",
    "Scalability and Efficiency: Training and deploying large-scale NLP models require significant computational resources and can be inefficient, making it challenging to scale.\n",
    "\n",
    "These challenges make NLP an exciting and continuously evolving field, with researchers and engineers actively working to address these issues and improve the performance, fairness, and interpretability of NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095bdbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
